<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>YOLOv8 WebAR Detection</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script src="https://aframe.io/releases/1.4.2/aframe.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/three@0.152.2/build/three.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.18.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl@4.18.0/dist/tf-backend-webgl.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
  <style>
    body { margin: 0; overflow: hidden; }
    #ar-scene { width: 100vw; height: 100vh; }
    #info { position: absolute; top: 10px; left: 10px; color: white; background: rgba(0,0,0,0.5); padding: 8px; border-radius: 4px; z-index: 2; }
  </style>
</head>
<body>
  <div id="info">Point your camera at an object. A plane will attach to detected items.</div>
  <a-scene embedded arjs="sourceType: webcam; debugUIEnabled: false;" id="ar-scene">
    <a-entity camera look-controls></a-entity>
    <a-entity id="detected-plane" visible="false">
      <a-plane color="#FF0000" width="1" height="1"></a-plane>
    </a-entity>
  </a-scene>
  <video id="video" autoplay playsinline style="position: fixed; top: 0; left: 0; width: 100vw; height: 100vh; object-fit: cover; z-index: 0;"></video>
  <script>
    // Request camera access on page load
    async function requestCamera() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: "environment" }, audio: false });
        const video = document.getElementById('video');
        video.srcObject = stream;
        await video.play();
      } catch (err) {
        alert('Camera access is required for AR detection.');
        console.error('Camera access denied:', err);
      }
    }

    // Use ONNX Runtime Web to run custom YOLOv8 model
    let session, video, detectedPlane, inputTensor, modelWidth = 640, modelHeight = 640;
    const onnxPath = 'best.onnx';

    async function setupDetection() {
      // Load ONNX model
      session = await ort.InferenceSession.create(onnxPath);
      video = document.getElementById('video');
      detectedPlane = document.querySelector('#detected-plane');
      detectFrame();
    }

    async function detectFrame() {
      if (!video || video.readyState !== 4) {
        requestAnimationFrame(detectFrame);
        return;
      }
      // Preprocess video frame to match model input
      const tfImg = tf.browser.fromPixels(video).resizeBilinear([modelHeight, modelWidth]).toFloat().div(255.0).expandDims(0);
      const input = tfImg.transpose([0, 3, 1, 2]); // BCHW
      const inputData = input.dataSync();
      const tensor = new ort.Tensor('float32', inputData, [1, 3, modelHeight, modelWidth]);
      // Run inference
      const results = await session.run({images: tensor});
      // Postprocess results (YOLOv8 output: [1, 5, 8400])
      const output = results[Object.keys(results)[0]].data;
      // Find the highest confidence detection
      let maxConf = 0, bestIdx = -1;
      for (let i = 0; i < 8400; i++) {
        const conf = output[i * 5 + 4];
        if (conf > maxConf) {
          maxConf = conf;
          bestIdx = i;
        }
      }
      if (maxConf > 0.3 && bestIdx !== -1) {
        // Extract box (x, y, w, h) in normalized coordinates
        const x = output[bestIdx * 5 + 0];
        const y = output[bestIdx * 5 + 1];
        const w = output[bestIdx * 5 + 2];
        const h = output[bestIdx * 5 + 3];
        // Map to [-1,1] for A-Frame positioning
        const nx = (x / modelWidth) * 2 - 1;
        const ny = -((y / modelHeight) * 2 - 1);
        detectedPlane.setAttribute('position', `${nx} ${ny} -2`);
        detectedPlane.setAttribute('scale', `${w/200} ${h/200} 1`);
        detectedPlane.setAttribute('visible', 'true');
      } else {
        detectedPlane.setAttribute('visible', 'false');
      }
      tfImg.dispose();
      input.dispose();
      requestAnimationFrame(detectFrame);
    }

    window.addEventListener('DOMContentLoaded', async () => {
      await requestCamera();
      setTimeout(setupDetection, 2000);
    });
    // Move the video element behind the AR scene
    document.getElementById('ar-scene').style.zIndex = 1;
  </script>
</body>
</html>
